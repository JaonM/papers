### Introduction
**Composition** <br>
1) the recurrent network encoder to build representation for questions and passages separately; <br>
2) the gated matching layer to match the question and passage; <br>
3) the self-matching layer to aggregate information from the whole passage; <br>
4) the pointer-network based answer boundary prediction layer.
### R-NET Structure
##### Overview
![r-net structure](./r-net structure.png)
* First,bi-rnn encodes questions and passage seperately.
* Second,match the question and passage with gate attention based rnn to acquire question-aware passage representation.
* Third,apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation
##### Question And Passage Encoder
Use both word-level embeddings of questions and passages $\{e^Q_t\}^m_{t=1}$ ,$\{e^P_t\}^n_{t=1}$ and character-level embedding $\{c^Q_t\}^m_{t=1}$ ,$\{c^P_t\}^n_{t=1}$ <br>
The character-level embeddings are generated by taking the final hidden
states of a bi-directional recurrent neural network (RNN) applied to embeddings of characters in the
token.Such  character-level can help deal with out of vocabulary problem. <br><br>
$u^Q_t=BiRNN_Q(u^Q_{t-1},[e^Q_t,c^Q_t])$ <br>
$u^P_t=BiRNN_P(u^P_{t-1},[e^P_t,c^P_t])$ 
##### Gated Attention-Based Reccurent Networks
sentence-pair representation $\{v^P_t\}^n_{t=1}$ <br>
$v^P_t=RNN(v^P_{t-1},c_t)$ where <br>
$c_t=att(u^Q,[u^P_t,v^P_{t-1}])$ an attention-pooling vector of the whole question($u^Q$) <br>
gated attention-based recurrent networks <br>
$g_t=sigmoid(W_g[u^P_t,c_t])$ <br>
$[u^P_t,c_t]^*=g_t\bigodot[u^P_t,c_t]$ <br>
The gate effectively model the phenomenon that only parts of the passage are relevant to the question in reading comprehension and question answering.
##### Self-Matching Attention
Dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation $h^P_t$ <br>
$h^P_t = BiRNN(h^P_{t-1},[v^P_t,c_t])$ <br>
where $c_t=att(v^P,v^P_t)$ is an attention-pooling vector of the whole passage($v^P$)  <br>
gated attention-based reccurrent network
$g_t=sigmoid(W_g[v^P_t,c_t])$ <br>
$[v^P_t,c_t]^*=g_t\bigodot[v^P_t,c_t]$ <br>
Self-matching extracts evidence from the whole passage according to the current passage word and
question information.
##### Output Layer

minimize the sum of the negative log probabilities of the ground truth start
and end position by the predicted distributions